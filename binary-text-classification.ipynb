{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "from ml_datasets import imdb\n",
    "import requests\n",
    "import csv\n",
    "train_data, valid_data = imdb()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ab93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_wikipedia():\n",
    "    url = \"https://olivers-things.s3.amazonaws.com/sample-wikipedia.json\"\n",
    "    articles = requests.get(url).json()\n",
    "    print(f\"fetched {len(articles)} articles\")\n",
    "    return articles\n",
    "\n",
    "def get_sample_wikipedia_untrained():\n",
    "    exclude = [a[2] for a in get_train_data()]\n",
    "    return [a for a in get_sample_wikipedia() if a[\"title\"] not in exclude]\n",
    "\n",
    "def get_train_data():\n",
    "    url = \"https://docs.google.com/spreadsheets/u/1/d/1ysgN8UoVY942gPVToxIEML_I-q27d9zTO6hAxoW5Yx4/export?format=csv&gid=0\"\n",
    "    tags = requests.get(url)\n",
    "    decoded = tags.content.decode('utf-8')\n",
    "    results = []\n",
    "    \n",
    "    for row in list(csv.reader(decoded.splitlines(), delimiter=','))[1:]:\n",
    "        try:\n",
    "            match = [a for a in articles if row[0] == a[\"title\"]][0]\n",
    "            title = match[\"title\"]\n",
    "            content = \" \".join(match[\"sentences\"])\n",
    "\n",
    "            if row[1] == \"0\":\n",
    "                results.append((content,\"neg\",title))\n",
    "            elif row[1] == \"1\":\n",
    "                results.append((content,\"pos\",title))\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {row[0]}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "def make_docs(data):\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels\n",
    "    and transform them in spacy documents\n",
    "    data: list(tuple(text, label))\n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    print(f\"training on {len(data)} docs\")\n",
    "    \n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        if label == 'neg':\n",
    "            doc.cats[\"positive\"] = 0\n",
    "            doc.cats[\"negative\"] = 1\n",
    "        else:\n",
    "            doc.cats[\"positive\"] = 1\n",
    "            doc.cats[\"negative\"] = 0\n",
    "\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bdf31",
   "metadata": {},
   "source": [
    "***\n",
    "***CREATE MODEL***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_sample_wikipedia()\n",
    "\n",
    "train_data = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeffb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_texts = 100\n",
    "# first we need to transform all the training data\n",
    "train_docs = make_docs(train_data[:num_texts])\n",
    "print(f\"training on {len(train_docs)} documents\")\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./data/train.spacy\")\n",
    "# repeat for validation data\n",
    "valid_docs = make_docs(valid_data[:num_texts])\n",
    "doc_bin = DocBin(docs=valid_docs)\n",
    "doc_bin.to_disk(\"./data/valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ce827",
   "metadata": {},
   "source": [
    "***\n",
    "***TEST MODEL***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aac2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained = get_sample_wikipedia_untrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89295716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_link(title):\n",
    "    return f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "\n",
    "for a in untrained[0:50]:\n",
    "    doc = nlp(\" \".join(a[\"sentences\"]))\n",
    "    result = 1 if doc.cats['positive'] > 0.5 else 0\n",
    "    print(wikipedia_link(a[\"title\"]), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471dda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f402433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b158bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a606d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "from flask import Flask, request\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"output/model-best\")\n",
    "\n",
    "def classify_wikipedia_article(article):\n",
    "    doc = nlp(\" \".join(article[\"sentences\"]))\n",
    "    article[\"result\"] = 1 if doc.cats['positive'] > 0.5 else 0\n",
    "    return article\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/classify\", methods=[\"POST\"])\n",
    "def classify():\n",
    "    classified = [classify_wikipedia_article(article) for article in request.json]\n",
    "    for article in classified:\n",
    "        if article[\"result\"] == 0:\n",
    "            print(f\"skipping {article['title']}\")\n",
    "        else:\n",
    "            print(f\"parsing {article['title']}\")\n",
    "    return \"hi\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_simple('localhost', 9000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37118e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
